{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "from collections import defaultdict, OrderedDict\n",
    "from os import path\n",
    "from transformers import BertTokenizerFast, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_file = \"/home/shtoshni/1023_bleak_house_brat.conll.txt\"\n",
    "litbank_dir = \"/home/shtoshni/Research/litbank_coref/litbank/coref/tsv\"\n",
    "ann_files = glob.glob(\"{}/*.ann\".format(litbank_dir))\n",
    "ann_files.sort()\n",
    "story_files = [ann_file.replace('.ann', '.txt') for ann_file in ann_files]\n",
    "\n",
    "output_dir = \"/home/shtoshni/Research/litbank_coref/data\"\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', add_special_tokens=False)\n",
    "\n",
    "sos_id = tokenizer.cls_token_id\n",
    "eos_id = tokenizer.sep_token_id\n",
    "\n",
    "max_segment_len = 350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sweet Glory: %d 350\n",
      "521_the_life_and_adventures_of_robinson_crusoe_brat\n",
      "Sweet Glory: %d 331\n",
      "521_the_life_and_adventures_of_robinson_crusoe_brat\n"
     ]
    }
   ],
   "source": [
    "proc_stories = []\n",
    "story_list = []\n",
    "for story_file, ann_file in zip(story_files, ann_files):\n",
    "    # The dictionary in which all the info is stored\n",
    "    proc_story = OrderedDict()\n",
    "    # Story name - Used as the key to doc\n",
    "    story_name = path.basename(story_file).replace(\".txt\", \"\")\n",
    "    story_list.append(path.basename(story_file))\n",
    "    proc_story[\"doc_key\"] = story_name\n",
    "    proc_story[\"sentences\"] = []\n",
    "    \n",
    "    # Load the story in a dictionary\n",
    "    with open(story_file) as f:\n",
    "        line_counter = 0\n",
    "        \n",
    "        # Doc level variables - Window of tokens\n",
    "        windows = []\n",
    "        subtoken_map = []\n",
    "        sent_map = []\n",
    "        \n",
    "        sent_counter = 0\n",
    "        token_counter = 0\n",
    "        \n",
    "        # Local variables\n",
    "        cur_window = []\n",
    "        \n",
    "        sent_to_subword_map = []\n",
    "        for line in f:\n",
    "            if len(cur_window) == 0:\n",
    "                # The old window of tokens has been released; starting with a fresh window of subtokens\n",
    "                sent_tokens = [sos_id]\n",
    "                sent_subtoken_map = [token_counter]\n",
    "                # word idx to subword idx mapping\n",
    "                word_to_subword_map = [0]\n",
    "            else:\n",
    "                sent_tokens = []\n",
    "                sent_subtoken_map = []\n",
    "                # word idx to subword idx mapping\n",
    "                word_to_subword_map = []\n",
    "            \n",
    "            words = line.strip().split(\" \")\n",
    "            for word_idx, word in enumerate(words):\n",
    "                token_ids = tokenizer.encode(word, add_special_tokens=False)\n",
    "                sent_tokens.extend(token_ids)\n",
    "\n",
    "                # Increase the sentence mapping\n",
    "                # Add the token to subtoken map\n",
    "                sent_subtoken_map.extend([token_counter] * len(token_ids))\n",
    "                word_to_subword_map.extend([word_idx] * len(token_ids))\n",
    "\n",
    "                token_counter += 1\n",
    "                \n",
    "            window_tokens = len(cur_window) + len(sent_tokens)\n",
    "            if window_tokens <= (max_segment_len - 1):  # reserving 1 for [SEP]\n",
    "                # The current sentence fits in the window\n",
    "                cur_window.extend(sent_tokens)\n",
    "                sent_map.extend([sent_counter] * len(sent_tokens))\n",
    "                subtoken_map.extend(sent_subtoken_map)\n",
    "                sent_to_subword_map.append(word_to_subword_map)\n",
    "                \n",
    "            else:\n",
    "                # Means that the current window wasn't empty.\n",
    "                # Need to add EOS ID to the last window before initializing the new window\n",
    "                cur_window.append(eos_id)\n",
    "                sent_map.append(sent_counter - 1)\n",
    "\n",
    "                # Repeat the word/token idx for EOS token\n",
    "                sent_to_subword_map[-1].append(sent_to_subword_map[-1][-1])\n",
    "                subtoken_map.append(subtoken_map[-1])\n",
    "\n",
    "                # Put the current window to the list of BERT-sentences/windows\n",
    "                proc_story[\"sentences\"].append(tokenizer.convert_ids_to_tokens(cur_window))\n",
    "\n",
    "                # Add [CLS] and the current sentence\n",
    "                cur_window = [sos_id] + sent_tokens\n",
    "                sent_map.extend([sent_counter] * (len(sent_tokens) + 1))\n",
    "                subtoken_map.extend([sent_subtoken_map[0]] + sent_subtoken_map)\n",
    "                sent_to_subword_map.append([0] + word_to_subword_map)\n",
    "                \n",
    "                if (len(word_to_subword_map) + 1) > max_segment_len:\n",
    "                    ### TODO: Handle a sentence longer than the max_segment_len\n",
    "                    print(\"Sweet Glory: %d\", len(word_to_subword_map) + 1)\n",
    "                    print(story_name)\n",
    "                \n",
    "            sent_counter += 1\n",
    "        \n",
    "        if cur_window:\n",
    "            cur_window.append(eos_id)\n",
    "            sent_map.append(sent_counter - 1)\n",
    "\n",
    "            # Repeat the word/token idx for EOS token\n",
    "            sent_to_subword_map[-1].append(sent_to_subword_map[-1][-1])\n",
    "            subtoken_map.append(subtoken_map[-1])\n",
    "            proc_story[\"sentences\"].append(tokenizer.convert_ids_to_tokens(cur_window))\n",
    "            \n",
    "        proc_story[\"sent_map\"] = sent_map\n",
    "        proc_story[\"subtoken_map\"] = subtoken_map\n",
    "        \n",
    "        proc_story[\"tokenized_sentences\"] = []\n",
    "        proc_story[\"tokenized_doc\"] = []\n",
    "        for sentence in proc_story[\"sentences\"]:\n",
    "            sent_tokens = tokenizer.convert_tokens_to_ids(sentence)\n",
    "            proc_story[\"tokenized_sentences\"].append(sent_tokens)\n",
    "            proc_story[\"tokenized_doc\"].extend(sent_tokens)\n",
    "            \n",
    "        sent_idx_to_subword_offset = [0]\n",
    "        subword_counter = 0\n",
    "        for idx, word_to_subword_map in enumerate(sent_to_subword_map):\n",
    "            num_subwords = len(word_to_subword_map)\n",
    "            subword_counter += num_subwords\n",
    "            sent_idx_to_subword_offset.append(subword_counter)\n",
    "        \n",
    "        assert(subword_counter == len(proc_story[\"subtoken_map\"]))       \n",
    "        assert(sum([len(sentence) for sentence in  proc_story[\"sentences\"]]) == len(proc_story[\"subtoken_map\"]))\n",
    "        \n",
    "        # Get the cluster information\n",
    "        with open(ann_file) as f:\n",
    "            mention_dict = {}\n",
    "            for line in f:\n",
    "                cols = line.strip().split(\"\\t\")\n",
    "                if cols[0] == 'MENTION':\n",
    "                    mention_id = cols[1]\n",
    "                    start_line, start_word_idx = int(cols[2]), int(cols[3])\n",
    "                    # Sentence offset\n",
    "                    start_line_offset = sent_idx_to_subword_offset[start_line]\n",
    "                    \n",
    "                    # Map the start word index to subword start index\n",
    "                    start_subword_idx = (start_line_offset + \n",
    "                                         sent_to_subword_map[start_line].index(start_word_idx))\n",
    "                    \n",
    "                    end_line, end_word_idx = int(cols[4]), int(cols[5])\n",
    "                    # Sentence offset\n",
    "                    end_line_offset = sent_idx_to_subword_offset[end_line]                    \n",
    "                    # Map the end word index to subword start index - Search for the last subword corresponding to it.\n",
    "                    \n",
    "                    end_word_start_idx = sent_to_subword_map[start_line].index(end_word_idx)\n",
    "                    end_word_idx_count = sent_to_subword_map[start_line].count(end_word_idx)\n",
    "                    end_subword_idx = (end_line_offset + end_word_start_idx + end_word_idx_count)\n",
    "                    \n",
    "                    given_mention_str = cols[6]\n",
    "                    ent_type, mention_type = cols[7], cols[8]\n",
    "\n",
    "                    assert(start_line == end_line)  # Check that no mention is across a line\n",
    "                    mention_dict[mention_id] = [start_subword_idx, end_subword_idx]\n",
    "    \n",
    "        with open(ann_file) as f:\n",
    "            coref_chains = OrderedDict()\n",
    "            for line in f:\n",
    "                cols = line.strip().split(\"\\t\")\n",
    "                if cols[0] == 'COREF':\n",
    "                    mention_id, cluster_id = cols[1], cols[2]\n",
    "                    if not cluster_id in coref_chains:\n",
    "                        coref_chains[cluster_id] = [mention_dict[mention_id]]\n",
    "                    else:\n",
    "                        coref_chains[cluster_id].append(mention_dict[mention_id])\n",
    "\n",
    "        proc_story[\"clusters\"] = [coref_chains[cluster_id] for cluster_id in coref_chains]\n",
    "        proc_stories.append(proc_story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set = (0, 80)\n",
    "# dev_set = (80, 88)\n",
    "# test_set = (88, 96)\n",
    "# final_set = (96, 100)\n",
    "\n",
    "# for (start_idx, end_idx), split in zip([train_set, dev_set, test_set, final_set], \n",
    "#                                        [\"train\", \"valid\", \"test\", \"final\"]):\n",
    "#     with open(path.join(output_dir, split + \".{}.jsonl\".format(max_segment_len)), \"w\") as f:\n",
    "#         for instance in proc_stories[start_idx: end_idx]:\n",
    "#             f.write(json.dumps(instance) + \"\\n\")\n",
    "            \n",
    "#     with open(path.join(output_dir, split + \".stories.txt\"), \"w\") as g:\n",
    "#         for story in story_list[start_idx: end_idx]:\n",
    "#             g.write(story + \"\\n\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path.join(output_dir, \"all.{}.jsonl\".format(max_segment_len)), \"w\") as f:\n",
    "    for instance in proc_stories:\n",
    "        f.write(json.dumps(instance) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:coref]",
   "language": "python",
   "name": "conda-env-coref-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
