{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from os import path\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "sys.path.append(\"/home/shtoshni/Research/long-doc-coref/src\")\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from inference.inference import Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/shtoshni/Research/litbank_coref/data/ontonotes/orig_independent/train.512.jsonlines']\n"
     ]
    }
   ],
   "source": [
    "input_dir = \"/home/shtoshni/Research/litbank_coref/data/ontonotes/orig_independent\"\n",
    "input_files = glob.glob(path.join(input_dir, \"*.jsonlines\"))\n",
    "input_files = [filename for filename in input_files if 'train.512' in filename]\n",
    "print(input_files)\n",
    "\n",
    "# output_dir = \"/home/shtoshni/Research/events/proc_data/kbp_2015/entity_chains\"\n",
    "# if not path.exists(output_dir):\n",
    "#     os.makedirs(output_dir)\n",
    "model_loc = \"/home/shtoshni/Research/long-doc-coref/models/umem_ontonotes/model.pth\"\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Inference(model_loc)\n",
    "# model = Inference(model_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"\"\"\n",
    "A man and a child have been killed after a light aircraft made an emergency landing\n",
    "on a beach in Portugal. Authorities said the incident took place on Sao Joao beach in Caparica, south-west\n",
    "of Lisbon.\n",
    "The National Maritime Authority said a middleaged man and a young girl died after they were unable to avoid the plane.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A man and a child have been killed after a light aircraft made an emergency landing\n",
      "on a beach in Portugal. Authorities said the incident took place on Sao Joao beach in Caparica, south-west\n",
      "of Lisbon.\n",
      "The National Maritime Authority said a middleaged man and a young girl died after they were unable to avoid the plane.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[((0, 4), 'A man and a child'), ((46, 52), 'a middleaged man and a young girl'), ((55, 55), 'they')], [((9, 11), 'a light aircraft'), ((60, 61), 'the plane')]]\n"
     ]
    }
   ],
   "source": [
    "print([cluster for cluster in model.perform_coreference(doc)['clusters'] if len(cluster) >= 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.perform_coreference(\"Kanye West was released from prison. He was caught with drugs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((0, 0), 'Abramov'), ((17, 17), 'Abramov'), ((26, 26), 'his'), ((239, 239), 'Abramov'), ((256, 256), 'Abramov'), ((265, 265), 'his')]\n",
      "[((6, 6), 'Moscow'), ((245, 245), 'Moscow')]\n",
      "[((7, 8), 'last night'), ((246, 247), 'last night')]\n",
      "[((34, 36), 'Chechen President Alkhanov'), ((46, 46), 'his'), ((273, 275), 'Chechen President Alkhanov'), ((285, 285), 'his')]\n",
      "[((38, 38), 'yesterday'), ((277, 277), 'yesterday')]\n",
      "[((46, 48), 'his car accident'), ((285, 287), 'his car accident')]\n",
      "[((70, 74), 'First Deputy Prime Minister Kadyrov'), ((309, 313), 'First Deputy Prime Minister Kadyrov')]\n",
      "[((89, 89), 'this'), ((328, 328), 'this')]\n",
      "[((91, 93), 'US President Bush'), ((142, 142), 'Bush'), ((176, 178), \"Bush ' s\"), ((330, 332), 'US President Bush'), ((381, 381), 'Bush'), ((415, 417), \"Bush ' s\")]\n",
      "[((91, 97), 'US President Bush and Russian President Putin'), ((124, 124), 'their'), ((142, 144), 'Bush and Putin'), ((190, 192), 'the two leaders'), ((330, 336), 'US President Bush and Russian President Putin'), ((363, 363), 'their'), ((381, 383), 'Bush and Putin'), ((429, 431), 'the two leaders')]\n",
      "[((95, 97), 'Russian President Putin'), ((144, 144), 'Putin'), ((334, 336), 'Russian President Putin'), ((383, 383), 'Putin')]\n",
      "[((100, 111), \"the 13th Informal APEC Leaders ' Meeting in Pusan , South Korea\"), ((117, 118), 'the meeting'), ((127, 128), 'the meeting'), ((183, 183), 'this'), ((339, 350), \"the 13th Informal APEC Leaders ' Meeting in Pusan , South Korea\"), ((356, 357), 'the meeting')]\n",
      "[((108, 111), 'Pusan , South Korea'), ((347, 350), 'Pusan , South Korea')]\n",
      "[((110, 111), 'South Korea'), ((349, 350), 'South Korea')]\n",
      "[((115, 115), 'today'), ((354, 354), 'today')]\n",
      "[((196, 197), 'Dear viewers'), ((210, 210), 'you'), ((435, 436), 'Dear viewers'), ((449, 449), 'you')]\n",
      "[((204, 207), 'the China News program'), ((443, 446), 'the China News program')]\n",
      "[((224, 224), 'us'), ((463, 463), 'us')]\n",
      "[((226, 233), 'the Focus Today program hosted by Wang Shilin'), ((465, 472), 'the Focus Today program hosted by Wang Shilin')]\n",
      "[((352, 352), 'met'), ((366, 367), 'the meeting'), ((422, 422), 'this')]\n"
     ]
    }
   ],
   "source": [
    "for input_file in input_files:\n",
    "#     output_file = path.join(output_dir, path.basename(input_file))\n",
    "    \n",
    "    with open(input_file) as input_f:#, open(output_file, \"w\") as output_f:\n",
    "        for line in input_f:\n",
    "            instance = json.loads(line.strip())\n",
    "            if instance[\"doc_key\"] == \"bc/cctv/00/cctv_0002_0\":\n",
    "                break\n",
    "                \n",
    "        \n",
    "        doc = \" \".join(instance[\"orig_tokens\"])\n",
    "        doc = doc +  \" \" + doc\n",
    "        \n",
    "        output_dict = model.perform_coreference(doc)\n",
    "        clusters = [cluster for cluster in output_dict[\"clusters\"] if len(cluster) > 1]\n",
    "        for cluster in clusters:\n",
    "            print(cluster)\n",
    "        \n",
    "#             doc = \"\"\n",
    "#             doc_len = 0\n",
    "#             word_idx_to_subtoken_maps = {}\n",
    "#             word_counter = 0\n",
    "#             for sentence in instance[\"tokenized_sentences\"]:\n",
    "#                 doc += \" \".join([word for word, _ in sentence]) + \" \"\n",
    "#                 doc_len += len(sentence)\n",
    "#                 for _, subtoken_span in sentence:\n",
    "#                     word_idx_to_subtoken_maps[word_counter] = subtoken_span\n",
    "#                     word_counter += 1\n",
    "                \n",
    "#             doc = doc.strip()\n",
    "#             output_dict = model.perform_coreference(doc)\n",
    "            \n",
    "#             all_tokens = []\n",
    "#             cur_subtoken_list = []\n",
    "#             cur_idx = 0\n",
    "            \n",
    "#             all_subtokens = []\n",
    "#             for sentence in output_dict[\"tokenized_doc\"][\"sentences\"]:\n",
    "#                 all_subtokens.extend(sentence)\n",
    "                \n",
    "#             for idx, subtoken_idx in enumerate(output_dict[\"tokenized_doc\"][\"subtoken_map\"]):\n",
    "#                 if subtoken_idx == cur_idx:\n",
    "#                     if all_subtokens[idx] not in ['[SEP]', '[CLS]']:\n",
    "#                         cur_subtoken_list.append(all_subtokens[idx])\n",
    "#                 else:\n",
    "#                     all_tokens.append(bert_tokenizer.convert_tokens_to_string(cur_subtoken_list))\n",
    "#                     if all_subtokens[idx] not in ['[SEP]', '[CLS]']:\n",
    "#                         cur_subtoken_list = [all_subtokens[idx]]\n",
    "#                     else:\n",
    "#                         cur_subtoken_list = []\n",
    "#                     cur_idx = subtoken_idx\n",
    "            \n",
    "            \n",
    "#             if cur_subtoken_list != []:\n",
    "#                 all_tokens.append(bert_tokenizer.convert_tokens_to_string(cur_subtoken_list))\n",
    "                         \n",
    "#             assert (doc_len == len(all_tokens))\n",
    "#             clusters = [cluster for cluster in output_dict[\"clusters\"] if len(cluster) > 1]\n",
    "#             entity_clusters = []\n",
    "#             for cluster in clusters:\n",
    "#                 entity_clusters.append([\n",
    "#                     (word_idx_to_subtoken_maps[start_idx][0], word_idx_to_subtoken_maps[end_idx][1], entity_str) \n",
    "#                     for (start_idx, end_idx), entity_str in cluster])\n",
    "                \n",
    "                \n",
    "                    \n",
    "# #             print(entity_clusters)\n",
    "            \n",
    "#             instance[\"entity_clusters\"] = entity_clusters\n",
    "#             output_f.write(json.dumps(instance) + \"\\n\")\n",
    "# #             break\n",
    "            \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:coref] *",
   "language": "python",
   "name": "conda-env-coref-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
