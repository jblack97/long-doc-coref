{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from os import path\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "sys.path.append(\"/home/shtoshni/Research/long-doc-coref/src\")\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from inference.inference import Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/shtoshni/Research/events/proc_data/kbp_2015/cleaned/train.512.jsonlines']\n"
     ]
    }
   ],
   "source": [
    "input_dir = \"/home/shtoshni/Research/events/proc_data/kbp_2015/cleaned\"\n",
    "input_files = glob.glob(path.join(input_dir, \"*.jsonlines\"))\n",
    "input_files = [filename for filename in input_files if 'train.512' in filename]\n",
    "print(input_files)\n",
    "\n",
    "output_dir = \"/home/shtoshni/Research/events/proc_data/kbp_2015/entity_chains\"\n",
    "if not path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "model_loc = \"/home/shtoshni/Research/long-doc-coref/models/umem_ontonotes/model.pth\"\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Inference(model_loc, device='cpu')\n",
    "# model = Inference(model_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.perform_coreference(\"Kanye West was released from prison. He was caught with drugs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_file in input_files:\n",
    "    output_file = path.join(output_dir, path.basename(input_file))\n",
    "    \n",
    "    with open(input_file) as input_f, open(output_file, \"w\") as output_f:\n",
    "        for line in input_f:\n",
    "            instance = json.loads(line.strip())\n",
    "            doc = \"\"\n",
    "            doc_len = 0\n",
    "            word_idx_to_subtoken_maps = {}\n",
    "            word_counter = 0\n",
    "            for sentence in instance[\"tokenized_sentences\"]:\n",
    "                doc += \" \".join([word for word, _ in sentence]) + \" \"\n",
    "                doc_len += len(sentence)\n",
    "                for _, subtoken_span in sentence:\n",
    "                    word_idx_to_subtoken_maps[word_counter] = subtoken_span\n",
    "                    word_counter += 1\n",
    "                \n",
    "            doc = doc.strip()\n",
    "            output_dict = model.perform_coreference(doc)\n",
    "            \n",
    "            all_tokens = []\n",
    "            cur_subtoken_list = []\n",
    "            cur_idx = 0\n",
    "            \n",
    "            all_subtokens = []\n",
    "            for sentence in output_dict[\"tokenized_doc\"][\"sentences\"]:\n",
    "                all_subtokens.extend(sentence)\n",
    "                \n",
    "            for idx, subtoken_idx in enumerate(output_dict[\"tokenized_doc\"][\"subtoken_map\"]):\n",
    "                if subtoken_idx == cur_idx:\n",
    "                    if all_subtokens[idx] not in ['[SEP]', '[CLS]']:\n",
    "                        cur_subtoken_list.append(all_subtokens[idx])\n",
    "                else:\n",
    "                    all_tokens.append(bert_tokenizer.convert_tokens_to_string(cur_subtoken_list))\n",
    "                    if all_subtokens[idx] not in ['[SEP]', '[CLS]']:\n",
    "                        cur_subtoken_list = [all_subtokens[idx]]\n",
    "                    else:\n",
    "                        cur_subtoken_list = []\n",
    "                    cur_idx = subtoken_idx\n",
    "            \n",
    "            \n",
    "            if cur_subtoken_list != []:\n",
    "                all_tokens.append(bert_tokenizer.convert_tokens_to_string(cur_subtoken_list))\n",
    "                         \n",
    "            assert (doc_len == len(all_tokens))\n",
    "            clusters = [cluster for cluster in output_dict[\"clusters\"] if len(cluster) > 1]\n",
    "            entity_clusters = []\n",
    "            for cluster in clusters:\n",
    "                entity_clusters.append([\n",
    "                    (word_idx_to_subtoken_maps[start_idx][0], word_idx_to_subtoken_maps[end_idx][1], entity_str) \n",
    "                    for (start_idx, end_idx), entity_str in cluster])\n",
    "                \n",
    "                \n",
    "                    \n",
    "#             print(entity_clusters)\n",
    "            \n",
    "            instance[\"entity_clusters\"] = entity_clusters\n",
    "            output_f.write(json.dumps(instance) + \"\\n\")\n",
    "#             break\n",
    "            \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:coref] *",
   "language": "python",
   "name": "conda-env-coref-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
