{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "from os import path\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0207 14:01:39.537658 140184527419200 tokenization_utils.py:373] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/shtoshni/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML_START = '<!DOCTYPE html><html lang=\"en\"><head><meta charset=\"UTF-8\"></head><body>'\n",
    "\n",
    "cluster_start_tag = '<div style=\"border:2px; display : inline; border-style:solid; padding: {}px; padding-right: 3px; padding-left: 3px\">'\n",
    "singleton_start_tag = '<div style=\"border:2px; display : inline; border-style:dotted; padding:{}px; padding-right: 3px; padding-left: 3px\">'\n",
    "end_tag = '</div>'\n",
    "\n",
    "largest_padding = 13\n",
    "padding_reduction = 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_html(json_file):\n",
    "    story_name = path.basename(story_file)\n",
    "    print(story_name)\n",
    "    \n",
    "    with open(json_file) as f:\n",
    "        doc = json.load(f)\n",
    "    \n",
    "    html_string = HTML_START + '<div style=\"line-height: 3\">'\n",
    "    \n",
    "    all_subtokens = []\n",
    "    for sentence in doc[\"tokenized_doc\"][\"sentences\"]:\n",
    "        all_subtokens.extend(sentence)\n",
    "    \n",
    "    all_tokens = []\n",
    "    cur_subtoken_list = []\n",
    "    cur_idx = 0\n",
    "    for idx, subtoken_idx in enumerate(doc[\"tokenized_doc\"][\"subtoken_map\"]):\n",
    "        if subtoken_idx == cur_idx:\n",
    "            if all_subtokens[idx] not in ['[SEP]', '[CLS]']:\n",
    "                cur_subtoken_list.append(all_subtokens[idx])\n",
    "        else:\n",
    "#             print(bert_tokenizer.convert_tokens_to_string(cur_subtoken_list))\n",
    "            all_tokens.append(bert_tokenizer.convert_tokens_to_string(cur_subtoken_list))\n",
    "            if all_subtokens[idx] not in ['[SEP]', '[CLS]']:\n",
    "                cur_subtoken_list = [all_subtokens[idx]]\n",
    "            else:\n",
    "                cur_subtoken_list = []\n",
    "            cur_idx = subtoken_idx\n",
    "            \n",
    "    \n",
    "#     print(all_tokens[:30])\n",
    "    if cur_subtoken_list != []:\n",
    "        all_tokens.append(bert_tokenizer.convert_tokens_to_string(cur_subtoken_list))\n",
    "    \n",
    "#     print(len(doc[\"tokenized_doc\"][\"subtoken_map\"]))\n",
    "    print(len(all_tokens), doc[\"tokenized_doc\"][\"subtoken_map\"][-1] + 1)\n",
    "#     print(len(all_subtokens))\n",
    "    \n",
    "    ment_start_dict = defaultdict(list)\n",
    "    ment_end_dict = defaultdict(list)\n",
    "    cluster_idx_to_len = defaultdict(int)\n",
    "    \n",
    "    clusters = sorted(doc[\"clusters\"], key=lambda x: x[0][0][0])\n",
    "    \n",
    "    for cluster_idx, ment_list in enumerate(clusters):\n",
    "        for (ment_start, ment_end), string in ment_list:\n",
    "#             try:\n",
    "#                 assert (ment_end > ment_start)\n",
    "#             except AssertionError:\n",
    "#                 print((ment_start, ment_end), string)\n",
    "            ment_start_dict[ment_start].append((ment_end, cluster_idx))\n",
    "            ment_end_dict[ment_end].append((ment_start, cluster_idx))\n",
    "            cluster_idx_to_len[cluster_idx] += 1\n",
    "                        \n",
    "    # Sort mentions with same mention start by later mention ends i.e. start with spans which are longer\n",
    "    for ment_start in ment_start_dict:\n",
    "        ment_start_dict[ment_start] = sorted(ment_start_dict[ment_start], key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "    # Sort mentions with same mention end by later mention starts i.e. start with spans which are shorter\n",
    "    for ment_end in ment_end_dict:\n",
    "        ment_end_dict[ment_end] = sorted(ment_end_dict[ment_end], key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "    active_clusters = 0\n",
    "    for token_idx, token in enumerate(all_tokens):\n",
    "        token_added = False\n",
    "        if token == \"\\n\":\n",
    "            html_string += \"<br/>\\n\" \n",
    "            continue\n",
    "        if token_idx in ment_start_dict:\n",
    "            for (_, cluster_idx) in ment_start_dict[token_idx]:\n",
    "                prefix = cluster_start_tag\n",
    "                if cluster_idx_to_len[cluster_idx] == 1:\n",
    "                    prefix = singleton_start_tag\n",
    "                html_string += prefix.format(largest_padding - active_clusters * padding_reduction)\n",
    "                active_clusters += 1\n",
    "            \n",
    "            html_string += token + \" \"\n",
    "            token_added = True\n",
    "        \n",
    "        if not token_added:\n",
    "            html_string += token + \" \"\n",
    "\n",
    "        if token_idx in ment_end_dict:\n",
    "            for (_, cluster_idx) in ment_end_dict[token_idx]:\n",
    "                html_string += \"<sub>\" + str(cluster_idx) + \"</sub>\" + end_tag + \" \"\n",
    "                active_clusters -= 1\n",
    "                assert (active_clusters >= 0)\n",
    "    \n",
    "    html_string += \"</div></body></html>\"\n",
    "    return html_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process all files to get HTML version of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ccarol_output.json\n",
      "41698 41698\n"
     ]
    }
   ],
   "source": [
    "story_file = \"/home/shtoshni/Research/long-doc-coref/notebooks/ccarol_output.json\"\n",
    "book_html = return_html(story_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_file = \"/home/shtoshni/Research/long-doc-coref/notebooks/ccarol_output.html\"\n",
    "with open(html_file, \"w\") as f:\n",
    "    f.write(book_html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:narrative_10]",
   "language": "python",
   "name": "conda-env-narrative_10-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
