{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "import json\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import glob\n",
    "from transformers import LongformerTokenizerFast\n",
    "# sys.path.append(\"../src/\")\n",
    "\n",
    "# from red_utils.constants import IDX_TO_ELEM_TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set input and output directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/shtoshni/Research/litbank_coref/data/ontonotes/ment_singletons_longformer/150.jsonlines', '/home/shtoshni/Research/litbank_coref/data/ontonotes/ment_singletons_longformer/90.jsonlines', '/home/shtoshni/Research/litbank_coref/data/ontonotes/ment_singletons_longformer/120.jsonlines', '/home/shtoshni/Research/litbank_coref/data/ontonotes/ment_singletons_longformer/30.jsonlines', '/home/shtoshni/Research/litbank_coref/data/ontonotes/ment_singletons_longformer/60.jsonlines']\n"
     ]
    }
   ],
   "source": [
    "# input_file = \"/home/shtoshni/Research/events/proc_data/kbp_2015\"\n",
    "# output_dir = \"/home/shtoshni/Research/events/data/kbp_2014-2015/bert_html\"\n",
    "input_dir = \"/home/shtoshni/Research/litbank_coref/data/ontonotes/ment_singletons_longformer\"\n",
    "base_output_dir = \"/home/shtoshni/Research/litbank_coref/data/ontonotes/ontonotes_ment_singletons_html\"\n",
    "\n",
    "train_file = \"/home/shtoshni/Research/litbank_coref/data/ontonotes/independent_longformer/train.2048.jsonlines\"\n",
    "tokenizer = LongformerTokenizerFast.from_pretrained('allenai/longformer-large-4096', add_prefix_space=True)\n",
    "singleton_files = glob.glob(path.join(input_dir, \"*.jsonlines\"))\n",
    "print(singleton_files)\n",
    "\n",
    "if not path.exists(base_output_dir):\n",
    "    os.makedirs(base_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_docs(file, k=10):\n",
    "    random.seed(20)\n",
    "    \n",
    "    doc_keys = set()\n",
    "    data = []\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            instance = json.loads(line.strip())\n",
    "            data.append(instance)\n",
    "            \n",
    "    random.shuffle(data)\n",
    "    data = data[:k]\n",
    "    data = {instance['doc_key']: instance for instance in data}\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML_START = '<!DOCTYPE html><html lang=\"en\"><head><meta charset=\"UTF-8\"></head><body>'\n",
    "\n",
    "\n",
    "start_tag_template = '<div style=\"border:2px; display:inline; border-style: {}; border-color: {}; padding: {}px; padding-right: 3px; padding-left: 3px\">'\n",
    "end_tag = '</div>'\n",
    "\n",
    "largest_padding = 13\n",
    "padding_reduction = 3\n",
    "\n",
    "\n",
    "def get_tag_options(cluster):\n",
    "    border = 'solid'\n",
    "    if len(cluster) == 1:\n",
    "        border = 'dotted'\n",
    "        \n",
    "    color = '#0066CC'\n",
    "        \n",
    "    return (border, color)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/shtoshni/Research/litbank_coref/data/ontonotes/ontonotes_ment_singletons_html/150/index.html\n",
      "/home/shtoshni/Research/litbank_coref/data/ontonotes/ontonotes_ment_singletons_html/90/index.html\n",
      "/home/shtoshni/Research/litbank_coref/data/ontonotes/ontonotes_ment_singletons_html/120/index.html\n",
      "/home/shtoshni/Research/litbank_coref/data/ontonotes/ontonotes_ment_singletons_html/30/index.html\n",
      "/home/shtoshni/Research/litbank_coref/data/ontonotes/ontonotes_ment_singletons_html/60/index.html\n"
     ]
    }
   ],
   "source": [
    "data = get_k_docs(train_file)\n",
    "\n",
    "for singleton_file in singleton_files:\n",
    "    subdir_name = path.basename(singleton_file).split(\".\")[0]\n",
    "    output_dir = path.join(base_output_dir, subdir_name)\n",
    "    if not path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    html_files = []\n",
    "    with open(singleton_file) as f:\n",
    "        doc_key_to_singleton_clusters = json.load(f)\n",
    "        for doc_key, instance in data.items():\n",
    "            if doc_key not in doc_key_to_singleton_clusters:\n",
    "                continue\n",
    "            bert_seg_idx = []\n",
    "            doc_list = [] \n",
    "            for sentence in instance[\"sentences\"]:\n",
    "                doc_list.extend(sentence)\n",
    "                bert_seg_idx.append(len(sentence) + (bert_seg_idx[-1] if len(bert_seg_idx) else 0))\n",
    "            \n",
    "            bert_seg_idx = set(bert_seg_idx)\n",
    "            html_tag_list = {}\n",
    "\n",
    "            # Get all the entity info\n",
    "#             from copy import deepcopy\n",
    "#             clusters = deepcopy(instance[\"clusters\"])\n",
    "#             clusters.extend(doc_key_to_singleton_clusters[doc_key])\n",
    "\n",
    "            clusters = doc_key_to_singleton_clusters[doc_key]\n",
    "            clusters = sorted(clusters, \n",
    "                              key=lambda cluster: min([elem[0] for elem in cluster]))\n",
    "            \n",
    "            for cluster_idx, cluster in enumerate(clusters):\n",
    "                cluster =  sorted(cluster, key=lambda ment: ment[0] - ment[1] * 1e-5)\n",
    "                for mention in cluster:\n",
    "                    span_start, span_end = mention\n",
    "                    span_end = span_end + 1  ## Now span_end is not part of the span\n",
    "                    \n",
    "                    if span_start not in html_tag_list:\n",
    "                        html_tag_list[span_start] = defaultdict(list)\n",
    "                    if span_end not in html_tag_list:\n",
    "                        html_tag_list[span_end] = defaultdict(list)\n",
    "\n",
    "                    subscript = str(cluster_idx)\n",
    "                    \n",
    "                    tag_options = get_tag_options(cluster)\n",
    "                    start_tag = start_tag_template.format(\n",
    "                        *tag_options, \n",
    "                        largest_padding - padding_reduction * len(html_tag_list[span_start]['start']))\n",
    "\n",
    "\n",
    "                    html_tag_list[span_start]['start'].append((start_tag))\n",
    "                    # Subscript used in end\n",
    "                    html_tag_list[span_end]['end'].append((span_start, cluster_idx, end_tag, subscript))\n",
    "\n",
    "\n",
    "            html_string = HTML_START + '<div style=\"line-height: 3\">'\n",
    "            for token_idx, token in enumerate(doc_list):\n",
    "                if token_idx in bert_seg_idx:\n",
    "                    html_string += \"\\n<br/>\"\n",
    "                    \n",
    "                if token_idx in html_tag_list:\n",
    "                    for tag_type in ['end', 'start']:\n",
    "                        if tag_type == 'end' and (tag_type in html_tag_list[token_idx]):\n",
    "                            tags = html_tag_list[token_idx]['end']\n",
    "\n",
    "                            # Sort the tags so as to mimic the stack behavior\n",
    "                            tags = sorted(tags, key=lambda x: x[0] - x[1] * 1e-5)  # Highest mentions first\n",
    "                            for _, _, html_tag, subscript in tags:\n",
    "                                html_string += \"<sub>\" + subscript + \"</sub>\" \n",
    "                                html_string += html_tag\n",
    "                                # Since we are deleting the highest indices first, the lower indices are unaffected\n",
    "                        \n",
    "                        \n",
    "                        if tag_type == 'start' and (tag_type in html_tag_list[token_idx]):\n",
    "                            tags = html_tag_list[token_idx]['start']\n",
    "                            tags = sorted(tags, key=lambda x: x[1], reverse=True)  # Highest mentions first\n",
    "                            for html_tag in html_tag_list[token_idx]['start']:\n",
    "                                html_string += html_tag\n",
    "\n",
    "                html_string += \" \" + tokenizer.convert_ids_to_tokens(token)\n",
    "\n",
    "            html_string += \"</div></body></html>\"\n",
    "            html_string = html_string.replace(\"\\n\", \"\\n<br/>\")\n",
    "            html_string = html_string.replace(\"~\", \"&lt;\")\n",
    "            html_string = html_string.replace(\"^\", \"&gt;\")\n",
    "            \n",
    "            file_name = f\"{len(doc_key_to_singleton_clusters[doc_key])} singletons - \" + instance[\"doc_key\"].replace(\"/\", \"-\") + \".html\"\n",
    "            file_path = path.join(output_dir, file_name)\n",
    "            html_files.append(file_name)\n",
    "            with open(file_path, \"w\") as f:\n",
    "                f.write(html_string)\n",
    "                \n",
    "                \n",
    "    index_html = HTML_START + '<ol type=\"1\">'\n",
    "\n",
    "    for html_file in html_files:\n",
    "        base_name = path.splitext(path.basename(html_file))[0].replace(\"-\", \"/\")\n",
    "        index_html += '<li> <a href=\"{}\", target=\"_blank\">'.format(html_file) + base_name + '</a></li>\\n'\n",
    "\n",
    "    index_html += '</ol>\\n</body>\\n</html>'\n",
    "    index_file_path = path.join(output_dir, \"index.html\")\n",
    "    print(index_file_path)\n",
    "    with open(path.join(output_dir, \"index.html\"), \"w\") as g:\n",
    "        g.write(index_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rap_nlp] *",
   "language": "python",
   "name": "conda-env-rap_nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
