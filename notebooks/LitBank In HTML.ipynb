{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "litbank_dir = \"../../lrec2020-coref/data/original/conll\"\n",
    "litbank_files = sorted(glob.glob(\"{}/*.conll\".format(litbank_dir)))\n",
    "\n",
    "output_dir = \"../../litbank_html\"\n",
    "assert(len(litbank_files) == 100)  # 100 documents in LitBank\n",
    "print(len(litbank_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process CoNLL formatted files to extract mention spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters(story_file):\n",
    "    story_name = path.basename(story_file)\n",
    "    with open(story_file) as f:\n",
    "        all_tokens = []\n",
    "        token_counter = 0\n",
    "        num_newline_tokens = 0\n",
    "        # Maintain list of all spans\n",
    "        cluster_id_to_spans = defaultdict(list)\n",
    "\n",
    "        # Maintain active clusters here\n",
    "        cluster_id_to_active_spans = defaultdict(list)\n",
    "        \n",
    "        for line_idx, line in enumerate(f.readlines()):\n",
    "            line = line.strip()\n",
    "            if line == \"\":\n",
    "                all_tokens.append(\"\\n\")\n",
    "                token_counter += 1\n",
    "                num_newline_tokens += 1\n",
    "                \n",
    "                # No active span crosses the sentence boundary.\n",
    "                assert(len(cluster_id_to_active_spans) == 0)\n",
    "            else:\n",
    "                cols = line.split(\"\\t\")\n",
    "                if len(cols) == 13:\n",
    "                    # Parse the cluster token. Examples - (38 or 38)|37) or (28|(26)\n",
    "                    cluster_token = cols[12]\n",
    "                    all_clusters = cluster_token.split('|')\n",
    "\n",
    "                    for cluster_str in all_clusters:\n",
    "                        if cluster_str[0] == '(' and cluster_str[-1] == ')':\n",
    "                            cluster_idx = int(cluster_str[1:-1])\n",
    "                            cluster_id_to_spans[cluster_idx].append([token_counter, token_counter])\n",
    "                        elif cluster_str[0] == '(':\n",
    "                            cluster_idx = int(cluster_str[1:])\n",
    "                            cluster_id_to_active_spans[cluster_idx].append(token_counter)\n",
    "                        elif cluster_str[-1] == ')':\n",
    "                            cluster_idx = int(cluster_str[:-1])\n",
    "                            assert (len(cluster_id_to_active_spans[cluster_idx]) > 0)\n",
    "                            start_idx = cluster_id_to_active_spans[cluster_idx].pop(-1)\n",
    "                            if len(cluster_id_to_active_spans[cluster_idx]) == 0:\n",
    "                                del cluster_id_to_active_spans[cluster_idx]\n",
    "                            cluster_id_to_spans[cluster_idx].append([start_idx, token_counter])\n",
    "                        else:\n",
    "                            print(\"Sweet Glory\")\n",
    "                            break\n",
    "                    \n",
    "                \n",
    "                if len(cols) >= 12:\n",
    "                    token = cols[3]\n",
    "                    all_tokens.append(token)\n",
    "                    token_counter += 1            \n",
    "                                \n",
    "    return cluster_id_to_spans, all_tokens, num_newline_tokens\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LitBank Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in LitBank: 210532\n",
      "# of Entity mentions: 29103, Total # of clusters: 7927\n",
      "Max clusters (940_the_last_of_the_mohicans_a_narrative_of_1757_brat.conll): 199\n",
      "Fraction of singleton clusters among total clusters: 0.73\n"
     ]
    }
   ],
   "source": [
    "total_tokens = 0\n",
    "total_mentions = 0\n",
    "total_clusters = 0\n",
    "singleton_clusters = 0\n",
    "\n",
    "max_clusters = 0\n",
    "max_cluster_story = None\n",
    "\n",
    "story_to_info = {}\n",
    "for story_file in litbank_files: \n",
    "    cluster_id_to_spans, all_tokens, num_newline_tokens = get_clusters(story_file)\n",
    "    story_to_info[story_file] = (all_tokens, cluster_id_to_spans)\n",
    "\n",
    "    total_tokens += len(all_tokens) - num_newline_tokens\n",
    "    total_clusters += len(cluster_id_to_spans)\n",
    "    \n",
    "    max_clusters = max(max_clusters, len(cluster_id_to_spans))\n",
    "    if max_clusters == len(cluster_id_to_spans):\n",
    "        max_cluster_story = path.basename(story_file)\n",
    "    \n",
    "    for cluster_id in cluster_id_to_spans:\n",
    "        cluster_mentions = len(cluster_id_to_spans[cluster_id])\n",
    "        total_mentions += cluster_mentions\n",
    "        if cluster_mentions == 1:\n",
    "            singleton_clusters += 1\n",
    "\n",
    "print(f\"Total tokens in LitBank: {total_tokens}\")\n",
    "print(f\"# of Entity mentions: {total_mentions}, Total # of clusters: {total_clusters}\")\n",
    "print(\"Max clusters ({}): {}\".format(max_cluster_story, max_clusters))\n",
    "print(f\"Fraction of singleton clusters among total clusters: {singleton_clusters/total_clusters:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML_START = '<!DOCTYPE html><html lang=\"en\"><head><meta charset=\"UTF-8\"></head><body>'\n",
    "\n",
    "cluster_start_tag = '<div style=\"border:2px; display : inline; border-style:solid; padding: {}px; padding-right: 3px; padding-left: 3px\">'\n",
    "singleton_start_tag = '<div style=\"border:2px; display : inline; border-style:dotted; padding:{}px; padding-right: 3px; padding-left: 3px\">'\n",
    "end_tag = '</div>'\n",
    "\n",
    "largest_padding = 11\n",
    "padding_reduction = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_html(story_file):\n",
    "    story_name = path.basename(story_file)\n",
    "    html_string = HTML_START + '<div style=\"line-height: 3\">'\n",
    "    \n",
    "    all_tokens, cluster_id_to_spans = story_to_info[story_file]\n",
    "    \n",
    "    ment_start_dict = defaultdict(list)\n",
    "    ment_end_dict = defaultdict(list)\n",
    "    for cluster_idx, ment_list in cluster_id_to_spans.items():\n",
    "        for (ment_start, ment_end) in ment_list:\n",
    "            ment_start_dict[ment_start].append((ment_end, cluster_idx))\n",
    "            ment_end_dict[ment_end].append((ment_start, cluster_idx))\n",
    "                        \n",
    "    # Sort mentions with same mention start by later mention ends i.e. start with spans which are longer\n",
    "    for ment_start in ment_start_dict:\n",
    "        ment_start_dict[ment_start] = sorted(ment_start_dict[ment_start], key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "    # Sort mentions with same mention end by later mention starts i.e. start with spans which are shorter\n",
    "    for ment_end in ment_end_dict:\n",
    "        ment_end_dict[ment_end] = sorted(ment_end_dict[ment_end], key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "    active_clusters = 0\n",
    "    for token_idx, token in enumerate(all_tokens):\n",
    "        token_added = False\n",
    "        if token == \"\\n\":\n",
    "            html_string += \"<br/>\\n\" \n",
    "            continue\n",
    "        if token_idx in ment_start_dict:\n",
    "            for (_, cluster_idx) in ment_start_dict[token_idx]:\n",
    "                prefix = cluster_start_tag\n",
    "                if len(cluster_id_to_spans[cluster_idx]) == 1:\n",
    "                    prefix = singleton_start_tag\n",
    "                html_string += prefix.format(largest_padding - active_clusters * padding_reduction)\n",
    "                active_clusters += 1\n",
    "            \n",
    "            html_string += token + \" \"\n",
    "            token_added = True\n",
    "        \n",
    "        if not token_added:\n",
    "            html_string += token + \" \"\n",
    "\n",
    "        if token_idx in ment_end_dict:\n",
    "            for (_, cluster_idx) in ment_end_dict[token_idx]:\n",
    "                html_string += \"<sub>\" + str(cluster_idx) + \"</sub>\" + end_tag + \" \"\n",
    "                active_clusters -= 1\n",
    "                assert (active_clusters >= 0)\n",
    "    \n",
    "    html_string += \"</div></body></html>\"\n",
    "    return html_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process all files to get HTML version of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../litbank_html/index.html\n"
     ]
    }
   ],
   "source": [
    "def extract_book_name(story_file):\n",
    "    conll_file = path.basename(story_file)\n",
    "    prefix = conll_file.split(\".\")[0]\n",
    "    prefix_words = prefix.split(\"_\")[1:-1]\n",
    "    book_name = (\" \".join(prefix_words)).capitalize()\n",
    "    \n",
    "    return book_name\n",
    "\n",
    "\n",
    "index_html = HTML_START + '<ol type=\"1\">'\n",
    "for story_file in litbank_files:\n",
    "    base_file = path.basename(story_file)\n",
    "    output_file = base_file.replace(\"conll\", \"html\")\n",
    "    output_file = output_file.replace(\"_brat\", \"\")\n",
    "    \n",
    "    book_name = extract_book_name(story_file)\n",
    "    index_html += '<li> <a href=\"{}\", target=\"_blank\">'.format(output_file) + book_name + '</a></li>\\n'\n",
    "    \n",
    "    book_html = return_html(story_file)\n",
    "    with open(path.join(output_dir, output_file), \"w\") as f:\n",
    "        f.write(book_html)\n",
    "        \n",
    "\n",
    "index_html += '</ol>\\n</body>\\n</html>'\n",
    "output_file = path.join(output_dir, \"index.html\")\n",
    "print(output_file)\n",
    "with open(output_file, \"w\") as g:\n",
    "    g.write(index_html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:narrative_10]",
   "language": "python",
   "name": "conda-env-narrative_10-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
