{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from metrics import CorefEvaluator\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"/home/shtoshni/Research/litbank_coref/models/ontonotes_logs\"\n",
    "\n",
    "models = [\"unbounded\", \"learned\", \"lru\"]\n",
    "num_cells = [\"5\", \"10\", \"20\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            data.append(json.loads(line.strip()))\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def mention_to_cluster(clusters, threshold=2):\n",
    "    clusters = [tuple(tuple(mention) for mention in cluster)\n",
    "                for cluster in clusters if len(cluster) >= threshold]\n",
    "    mention_to_cluster = {}\n",
    "    for cluster in clusters:\n",
    "        for mention in cluster:\n",
    "            mention_to_cluster[mention] = cluster\n",
    "    return clusters, mention_to_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load logs here\n",
    "\n",
    "model_to_logs = {}\n",
    "for model in models:\n",
    "    if model == 'unbounded':\n",
    "        model_file = path.join(log_dir, \"ontonotes_unbounded.jsonl\")\n",
    "        model_to_logs['unbounded'] = load_jsonl(model_file)\n",
    "    if model != 'unbounded':\n",
    "        for num_cell in num_cells:\n",
    "            model_file = path.join(log_dir, \"ontonotes_{}_{}.jsonl\".format(model, num_cell))\n",
    "            model_to_logs['{}_{}'.format(model, num_cell)] = load_jsonl(model_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['unbounded', 'learned_5', 'learned_10', 'learned_20', 'lru_5', 'lru_10', 'lru_20'])\n"
     ]
    }
   ],
   "source": [
    "print(model_to_logs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overwrites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unbounded, max: 87, avg: 16.3\n",
      "learned_5, max: 5, avg: 4.6\n",
      "learned_10, max: 10, avg: 8.1\n",
      "learned_20, max: 20, avg: 12.4\n",
      "lru_5, max: 5, avg: 4.6\n",
      "lru_10, max: 10, avg: 8.1\n",
      "lru_20, max: 20, avg: 12.4\n"
     ]
    }
   ],
   "source": [
    "for model, logs in model_to_logs.items():\n",
    "    mem_usage = []\n",
    "    for log in logs:\n",
    "        over_action = sum([1  for action in log[\"pred_actions\"] if action[1]=='o' ])\n",
    "    \n",
    "        if 'unbounded' not in model:\n",
    "            num_cells = int(model.split('_')[-1])\n",
    "            over_action = min(over_action, num_cells)\n",
    "\n",
    "        mem_usage.append(over_action) \n",
    "    \n",
    "    \n",
    "    print ('{}, max: {}, avg: {:.1f}'.format(model, max(mem_usage), np.mean(mem_usage)))\n",
    "    \n",
    "#     print(mem_usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ignored Mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unbounded, max: 0, avg: 0.0\n",
      "learned_5, max: 21, avg: 0.5\n",
      "learned_10, max: 1, avg: 0.0\n",
      "learned_20, max: 1, avg: 0.0\n",
      "lru_5, max: 79, avg: 5.4\n",
      "lru_10, max: 24, avg: 0.7\n",
      "lru_20, max: 3, avg: 0.0\n"
     ]
    }
   ],
   "source": [
    "for model, logs in model_to_logs.items():\n",
    "    mem_usage = []\n",
    "    for log in logs:\n",
    "        over_action = sum([1  for action in log[\"pred_actions\"] if action[1]=='n' ])\n",
    "    \n",
    "        if 'unbounded' not in model:\n",
    "            num_cells = int(model.split('_')[-1])\n",
    "#             over_action = min(over_action, num_cells)\n",
    "\n",
    "        mem_usage.append(over_action) \n",
    "    \n",
    "    \n",
    "    print ('{}, max: {}, avg: {:.1f}'.format(model, max(mem_usage), np.mean(mem_usage)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spearman Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unbounded doc len -0.31\n",
      "unbounded num ent -0.27\n",
      "learned_5 doc len -0.38\n",
      "learned_5 num ent -0.39\n",
      "learned_10 doc len -0.37\n",
      "learned_10 num ent -0.35\n",
      "learned_20 doc len -0.30\n",
      "learned_20 num ent -0.27\n",
      "lru_5 doc len -0.42\n",
      "lru_5 num ent -0.47\n",
      "lru_10 doc len -0.36\n",
      "lru_10 num ent -0.37\n",
      "lru_20 doc len -0.33\n",
      "lru_20 num ent -0.30\n"
     ]
    }
   ],
   "source": [
    "model_perf_per_example = {}\n",
    "for model in model_to_logs:\n",
    "    log_data = model_to_logs[model]\n",
    "    perf_list = []\n",
    "    for example in log_data:\n",
    "        evaluator = CorefEvaluator()\n",
    "                \n",
    "        predicted_clusters, mention_to_predicted =\\\n",
    "            mention_to_cluster(example[\"predicted_clusters\"], threshold=2)\n",
    "        gold_clusters, mention_to_gold =\\\n",
    "            mention_to_cluster(example[\"clusters\"], threshold=2)\n",
    "\n",
    "\n",
    "        evaluator.update(predicted_clusters, gold_clusters,\n",
    "                         mention_to_predicted, mention_to_gold)\n",
    "        \n",
    "        doc_len = example[\"subtoken_map\"][-1] + 1\n",
    "        num_ents = len(gold_clusters)\n",
    "        \n",
    "        example_fscore = evaluator.get_prf()[2] * 100.0\n",
    "        \n",
    "        perf_list.append((doc_len, num_ents, example_fscore))\n",
    "        \n",
    "        \n",
    "    model_perf_per_example[model] = perf_list\n",
    "\n",
    "for model in model_perf_per_example:\n",
    "    perf_list = model_perf_per_example[model]\n",
    "    doc_len_list, num_ent_list, fscore = zip(*perf_list)\n",
    "    \n",
    "    print('{} doc len {:.2f}'.format(model, spearmanr(doc_len_list, fscore)[0]))\n",
    "    print('{} num ent {:.2f}'.format(model, spearmanr(num_ent_list, fscore)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucketing examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the unbounded mem examples\n",
    "\n",
    "data = model_to_logs['unbounded']\n",
    "doc_key_len_list = [(example[\"doc_key\"], example[\"subtoken_map\"][-1] + 1) \n",
    "                    for example in data]\n",
    "\n",
    "doc_key_len_list = sorted(doc_key_len_list, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions = []\n",
    "len_constrainst = [(0, 128), (129, 256), (257, 512), (513, 768), (769, 1152), (1153, np.inf)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unbounded doc len -0.31\n",
      "unbounded num ent -0.27\n",
      "learned_5 doc len -0.38\n",
      "learned_5 num ent -0.39\n",
      "learned_10 doc len -0.37\n",
      "learned_10 num ent -0.35\n",
      "learned_20 doc len -0.30\n",
      "learned_20 num ent -0.27\n",
      "lru_5 doc len -0.42\n",
      "lru_5 num ent -0.47\n",
      "lru_10 doc len -0.36\n",
      "lru_10 num ent -0.37\n",
      "lru_20 doc len -0.33\n",
      "lru_20 num ent -0.30\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "model_perf_per_example = {}\n",
    "for model in model_to_logs:\n",
    "    log_data = model_to_logs[model]\n",
    "    perf_list = []\n",
    "    for example in log_data:\n",
    "        evaluator = CorefEvaluator()\n",
    "                \n",
    "        predicted_clusters, mention_to_predicted =\\\n",
    "            mention_to_cluster(example[\"predicted_clusters\"], threshold=2)\n",
    "        gold_clusters, mention_to_gold =\\\n",
    "            mention_to_cluster(example[\"clusters\"], threshold=2)\n",
    "\n",
    "\n",
    "        evaluator.update(predicted_clusters, gold_clusters,\n",
    "                         mention_to_predicted, mention_to_gold)\n",
    "        \n",
    "        doc_len = example[\"subtoken_map\"][-1] + 1\n",
    "        num_ents = len(gold_clusters)\n",
    "        \n",
    "        example_fscore = evaluator.get_prf()[2] * 100.0\n",
    "        \n",
    "        perf_list.append((doc_len, num_ents, example_fscore))\n",
    "        \n",
    "        \n",
    "    model_perf_per_example[model] = perf_list\n",
    "\n",
    "for model in model_perf_per_example:\n",
    "    perf_list = model_perf_per_example[model]\n",
    "    doc_len_list, num_ent_list, fscore = zip(*perf_list)\n",
    "    \n",
    "    print('{} doc len {:.2f}'.format(model, spearmanr(doc_len_list, fscore)[0]))\n",
    "    print('{} num ent {:.2f}'.format(model, spearmanr(num_ent_list, fscore)[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:narrative_10]",
   "language": "python",
   "name": "conda-env-narrative_10-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
